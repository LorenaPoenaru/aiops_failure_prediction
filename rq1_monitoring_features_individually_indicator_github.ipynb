{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e488916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_natural_chunks(features, labels, terminals):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(terminals) - 1):\n",
    "        idx = np.logical_and(features[:, 0] >= terminals[i], features[:, 0] < terminals[i + 1])\n",
    "        feature_list.append(features[idx][:, 1:])\n",
    "        label_list.append(labels[idx])\n",
    "    return feature_list, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d56731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_intervals(dataset):\n",
    "    '''\n",
    "    Generate interval terminals, so that samples in each interval have:\n",
    "        interval_i = (timestamp >= terminal_i) and (timestamp < terminal_{i+1})\n",
    "\n",
    "    Args:\n",
    "        dataset (chr): Assuming only Backblaze (b) and Google (g) datasets exists\n",
    "    '''\n",
    "    if dataset == 'g':\n",
    "        # time unit in Google: millisecond, tracing time: 29 days\n",
    "        start_time = 604046279\n",
    "        unit_period = 24 * 60 * 60 * 1000 * 1000  # unit period: one day\n",
    "        end_time = start_time + 28*unit_period\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: week, tracing time: one year (50 weeks)\n",
    "        start_time = 1\n",
    "        unit_period = 7  # unit period: one week (7 days)\n",
    "        end_time = start_time + 50*unit_period\n",
    "    # original 1 month\n",
    "    '''\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: month, tracing time: one year (12 months)\n",
    "        start_time = 1\n",
    "        unit_period = 1  # unit period: one month\n",
    "        end_time = start_time + 12*unit_period\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # add one unit for the open-end of range function\n",
    "    terminals = [i for i in range(start_time, end_time+unit_period, unit_period)]\n",
    "\n",
    "    return terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_labels_extraction(dataset, data_path):\n",
    "    \n",
    "    # currently assume only b (disk) and g (job) data exist\n",
    "    if(dataset=='b'):\n",
    "\n",
    "        \n",
    "        # define features\n",
    "        features_disk_failure = ['smart_1_raw', 'smart_4_raw', 'smart_5_raw', 'smart_7_raw', 'smart_9_raw', 'smart_12_raw', 'smart_187_raw', 'smart_193_raw', 'smart_194_raw', 'smart_197_raw', 'smart_199_raw', \n",
    "                         'smart_4_raw_diff', 'smart_5_raw_diff', 'smart_9_raw_diff', 'smart_12_raw_diff', 'smart_187_raw_diff', 'smart_193_raw_diff', 'smart_197_raw_diff', 'smart_199_raw_diff']\n",
    "        columns = ['serial_number', 'date'] + features_disk_failure + ['label']\n",
    "        \n",
    "        # read data\n",
    "        df = pd.read_csv(data_path, header=None)\n",
    "        # put columns names\n",
    "        df.columns = columns\n",
    "        # ignore serial number\n",
    "        df = df[df.columns[1:]]\n",
    "        \n",
    "        # transform date to date time\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "        \n",
    "        # divide on weeks\n",
    "        df['date'] = pd.Series(pd.DatetimeIndex(df['date']).day_of_year)\n",
    "        \n",
    "        # extract features and labels\n",
    "        features = df[df.columns[:-1]].to_numpy()\n",
    "        labels = df[df.columns[-1]].to_numpy()\n",
    "        \n",
    "        feature_list, label_list = obtain_natural_chunks(features, labels, obtain_intervals('b'))\n",
    "        \n",
    "    elif(dataset == 'g'):\n",
    "        \n",
    "        # define features\n",
    "        features_job_failure = ['User ID', 'Job Name', 'Scheduling Class',\n",
    "                   'Num Tasks', 'Priority', 'Diff Machine', 'CPU Requested', 'Mem Requested', 'Disk Requested',\n",
    "                   'Avg CPU', 'Avg Mem', 'Avg Disk', 'Std CPU', 'Std Mem', 'Std Disk']\n",
    "        columns_initial = ['Job ID', 'Status', 'Start Time', 'End Time'] + features_job_failure\n",
    "        \n",
    "        # read data\n",
    "        df = pd.read_csv(data_path, header=None)\n",
    "        # put columns names\n",
    "        df.columns = columns_initial\n",
    "        df = df.tail(-1)\n",
    "\n",
    "        # drop Job ID\n",
    "        df = df.drop(['Job ID'], axis = 1)\n",
    "        \n",
    "        # get features\n",
    "        columns = features_job_failure\n",
    "        \n",
    "        include_end_time = False\n",
    "        \n",
    "        # EXTRACT FEATURES AND LABELS\n",
    "        features = df[(['Start Time']+ features_job_failure)].to_numpy()\n",
    "        labels = (df['Status']==3).to_numpy()\n",
    "\n",
    "\n",
    "        # FEATURES PREPROCESSING\n",
    "        offset = (1 if include_end_time else 0)\n",
    "\n",
    "        # ENCODE USER ID\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        features[:, 1+offset] = le.fit_transform(features[:, 1+offset])\n",
    "\n",
    "        # ENCODE JOB NAME\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        features[:, 2+offset] = le.fit_transform(features[:, 2+offset])\n",
    "\n",
    "        features = features.astype(float)\n",
    "        \n",
    "        feature_list, label_list = obtain_natural_chunks(features, labels, obtain_intervals('g'))\n",
    "\n",
    "    else:\n",
    "        print('Undefined value')\n",
    "    return feature_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980719aa",
   "metadata": {},
   "source": [
    "We assume that only Backblaze (disk failure) - 'b' and Google (job failure) - 'g' datasets exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec198216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset to 'g' for experiments with Google data (job) and 'b' for experiments with Backblaze data (disk)\n",
    "dataset = 'g'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba1d3e",
   "metadata": {},
   "source": [
    "# Reading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == 'b'):\n",
    "    DATASET_PATH = './datasets/disk_failure_2015.csv'\n",
    "elif(dataset == 'g'):\n",
    "    DATASET_PATH = './datasets/google_job_failure.csv'\n",
    "\n",
    "feature_list, label_list = features_labels_extraction(dataset, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85be0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18334889",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380c401",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4f8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(dataset == 'b'):\n",
    "    df_concept_drift = pd.read_csv('../../Documents/phd_related/AIOps_disk_failure_prediction/feature_importance/rf_concept_drift_localization_disk_week_2015_r_1.csv')\n",
    "elif(dataset == 'g'):\n",
    "    df_concept_drift = pd.read_csv('../../Documents/phd_related/AIOps_disk_failure_prediction/feature_importance/rf_concept_drift_localization_job_r_1.csv')\n",
    "df_concept_drift = df_concept_drift.loc[:, ~df_concept_drift.columns.str.contains('^Unnamed')]\n",
    "df_concept_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "stat_test = stats.kstest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa4cc8",
   "metadata": {},
   "source": [
    "# Monitoring the No of Features that Change in Each Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_changed_features_per_period = []\n",
    "no_changed_features_per_period = []\n",
    "\n",
    "for period in tqdm(range(0, len(feature_list)-1)):\n",
    "\n",
    "    # extract features train and test\n",
    "    training_features = scaler.fit_transform(feature_list[period])\n",
    "    testing_features = scaler.transform(feature_list[period+1])\n",
    "    \n",
    "    # convert numpy array to Pandas Dataframe\n",
    "    df_train_features = pd.DataFrame(training_features, columns = features_disk_failure)\n",
    "    df_test_features = pd.DataFrame(testing_features, columns = features_disk_failure)\n",
    "    \n",
    "    no_changed_features = 0\n",
    "    for feature in features_disk_failure:\n",
    "        #print(feature)\n",
    "        #print(df_train_features[feature])\n",
    "        #print(df_test_features[feature])\n",
    "        v, p = stat_test(df_train_features[feature], df_test_features[feature])\n",
    "        if(p<0.05):\n",
    "            no_changed_features = no_changed_features + 1\n",
    "    \n",
    "    no_changed_features_per_period.append(no_changed_features)\n",
    "    perc_changed_features_per_period.append(no_changed_features/len(features_disk_failure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc53561",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(perc_changed_features_per_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# period\n",
    "if(dataset == 'b'):\n",
    "    period = []\n",
    "    for i in range(0, len(feature_list)-1):\n",
    "        string_period = 'W' + str(i+1) + '_' + str(i+2)\n",
    "        period.append(string_period)\n",
    "elif(dataset == 'g'):\n",
    "    for i in range(0, len(feature_list)-1):\n",
    "        string_period = 'D' + str(i+1) + '_' + str(i+2)\n",
    "        period.append(string_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_grount_truth = list(df_concept_drift.Sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff38dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with final results\n",
    "\n",
    "df_results_monitoring_all_individual_features = pd.DataFrame()\n",
    "df_results_monitoring_all_individual_features['Period'] = period\n",
    "df_results_monitoring_all_individual_features['Drift Ground Truth'] = drift_grount_truth\n",
    "df_results_monitoring_all_individual_features['No Changed Features Per Period'] = no_changed_features_per_period\n",
    "df_results_monitoring_all_individual_features['Percentage Changed Features Per Period'] = perc_changed_features_per_period\n",
    "df_results_monitoring_all_individual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(dataset == 'b'):\n",
    "    df_results_monitoring_all_individual_features.to_csv('./results/df_percentage_of_changed_features_disk.csv')\n",
    "elif(dataset == 'g'):\n",
    "    df_results_monitoring_all_individual_features.to_csv('./results/df_percentage_of_changed_features_job.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1cf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
