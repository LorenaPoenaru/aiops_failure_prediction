{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f970d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a383bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seeds = [1234, 4887, 597, 1959, 413, 44, 2969, 4971, 4913, 9591]\n",
    "# example with random seed 1234\n",
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_intervals(dataset):\n",
    "    '''\n",
    "    Generate interval terminals, so that samples in each interval have:\n",
    "        interval_i = (timestamp >= terminal_i) and (timestamp < terminal_{i+1})\n",
    "\n",
    "    Args:\n",
    "        dataset (chr): Assuming only Backblaze (b) and Google (g) datasets exists\n",
    "    '''\n",
    "    if dataset == 'g':\n",
    "        # time unit in Google: millisecond, tracing time: 29 days\n",
    "        start_time = 604046279\n",
    "        unit_period = 24 * 60 * 60 * 1000 * 1000  # unit period: one day\n",
    "        end_time = start_time + 28*unit_period\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: month, tracing time: one year (12 months)\n",
    "        start_time = 1\n",
    "        unit_period = 1  # unit period: one month\n",
    "        end_time = start_time + 12*unit_period\n",
    "\n",
    "    # add one unit for the open-end of range function\n",
    "    terminals = [i for i in range(start_time, end_time+unit_period, unit_period)]\n",
    "\n",
    "    return terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_model(model_name):\n",
    "    '''\n",
    "    This function instantiate a specific model \n",
    "    Note: the MODEL_TYPE global variable must be set first\n",
    "    Args:\n",
    "        model_name (str): [rf, nn, svm, cart, rgf]\n",
    "    Returns:\n",
    "        (instance): instance of given model with preset parameters.\n",
    "        Return None if the model name is not in the option\n",
    "    '''\n",
    "    if model_name == 'rf':\n",
    "        return RandomForestClassifier(n_estimators=50, criterion='gini', class_weight=None, max_depth=None, \n",
    "                                      min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, \n",
    "                                      n_jobs=N_WORKERS, random_state = random_seed)\n",
    "        #return RandomForestClassifier(n_jobs=N_WORKERS)\n",
    "    elif model_name == 'nn':\n",
    "        return MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, learning_rate='adaptive')\n",
    "        #return MLPClassifier()\n",
    "    elif model_name == 'svm':\n",
    "        return SVC(max_iter=100000, probability=True)\n",
    "        #return SVC(max_iter=10000, probability=True)\n",
    "    elif model_name == 'cart':\n",
    "        return DecisionTreeClassifier(criterion='gini', class_weight=None, max_depth=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2)\n",
    "        #return DecisionTreeClassifier()\n",
    "    elif model_name == 'rgf':\n",
    "        return SafeRGF()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ae09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_natural_chunks(features, labels, terminals):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(terminals) - 1):\n",
    "        idx = np.logical_and(features[:, 0] >= terminals[i], features[:, 0] < terminals[i + 1])\n",
    "        feature_list.append(features[idx][:, 1:])\n",
    "        label_list.append(labels[idx])\n",
    "    return feature_list, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_original(model, features, labels):\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    error_num = 0\n",
    "    total_num = 0\n",
    "    for training_index, testing_index in kf.split(features):\n",
    "        training_features, training_labels = features[training_index], labels[training_index]\n",
    "        testing_features, testing_labels = features[testing_index], labels[testing_index]\n",
    "        model.fit(training_features, training_labels)\n",
    "        testing_preds = model.predict(testing_features)\n",
    "        error_num += np.count_nonzero(testing_preds != testing_labels)\n",
    "        total_num += len(testing_labels)\n",
    "\n",
    "    return error_num, total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80deea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "def cross_validation(model, features, labels):\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    error_num = 0\n",
    "    total_num = 0\n",
    "    testing_preds_cross_val = []\n",
    "    testing_true_cross_val = []\n",
    "    \n",
    "    counter = 0;\n",
    "    \n",
    "    for training_index, testing_index in kf.split(features):\n",
    "        \n",
    "        counter = counter + 1\n",
    "        #print(counter)\n",
    "        \n",
    "        training_features, training_labels = features[training_index], labels[training_index]\n",
    "        testing_features, testing_labels = features[testing_index], labels[testing_index]\n",
    "        model.fit(training_features, training_labels)\n",
    "        testing_preds = model.predict(testing_features)\n",
    "        \n",
    "        testing_preds_cross_val.append(testing_preds)\n",
    "        testing_true_cross_val.append(testing_labels)\n",
    "        \n",
    "        \n",
    "        error_num += np.count_nonzero(testing_preds != testing_labels)\n",
    "        total_num += len(testing_labels)\n",
    "\n",
    "        \n",
    "    return testing_true_cross_val, testing_preds_cross_val, error_num, total_num\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91152054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aa2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './datasets/google_job_failure.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbd316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb4ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720bc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_job_failure = ['User ID', 'Job Name', 'Scheduling Class',\n",
    "                   'Num Tasks', 'Priority', 'Diff Machine', 'CPU Requested', 'Mem Requested', 'Disk Requested',\n",
    "                   'Avg CPU', 'Avg Mem', 'Avg Disk', 'Std CPU', 'Std Mem', 'Std Disk']\n",
    "columns_initial = ['Job ID', 'Status', 'Start Time', 'End Time'] + features_job_failure\n",
    "\n",
    "\n",
    "# READ DATA\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, header=None)\n",
    "df.columns = columns_initial\n",
    "df = df.tail(-1)\n",
    "\n",
    "df = df.drop(['Job ID'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_end_time = False\n",
    "\n",
    "# EXTRACT FEATURES AND LABELS\n",
    "\n",
    "features = df[(['Start Time']+ features_job_failure)].to_numpy()\n",
    "labels = (df['Status']==3).to_numpy()\n",
    "\n",
    "\n",
    "# FEATURES PREPROCESSING\n",
    "offset = (1 if include_end_time else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE USER ID\n",
    "le = preprocessing.LabelEncoder()\n",
    "features[:, 1+offset] = le.fit_transform(features[:, 1+offset])\n",
    "\n",
    "# ENCODE JOB NAME\n",
    "le = preprocessing.LabelEncoder()\n",
    "features[:, 2+offset] = le.fit_transform(features[:, 2+offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDE FEATURES INTO DAYS \n",
    "\n",
    "feature_list, label_list = obtain_natural_chunks(features, labels, obtain_intervals('g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3173b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81605f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6bc15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataframe to store results\n",
    "df_expected_actual = pd.DataFrame()\n",
    "df_expected_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['D1_2', 'D2_3', 'D3_4', 'D4_5', 'D5_6', 'D6_7', 'D7_8', 'D8_9', 'D9_10', 'D10_11',\n",
    "        'D11_12', 'D12_13', 'D13_14', 'D14_15', 'D15_16', 'D16_17', 'D17_18', 'D18_19', 'D20_21',\n",
    "        'D22_23', 'D23_24', 'D24_25', 'D25_26', 'D26_27', 'D27_28', 'D28_29', 'D29_30']\n",
    "len(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a513b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b52a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711deeaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'rf'\n",
    "N_WORKERS = 1\n",
    "\n",
    "training_error_rate = []\n",
    "testing_error_rate = []\n",
    "training_size = []\n",
    "testing_size = []\n",
    "\n",
    "#expected_roc_auc_cross_val = []\n",
    "\n",
    "#actual_roc_auc = []\n",
    "\n",
    "feature_importance = []\n",
    "\n",
    "for i in tqdm(range(0, len(days))):\n",
    "        \n",
    "    # OBTAIN TRAINING DATA + LABELS\n",
    "    day = i\n",
    "    print('Train day', day)\n",
    "    \n",
    "    \n",
    "    # SCALE TRAINING DATA\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    training_features = scaler.fit_transform(feature_list[day])\n",
    "    \n",
    "    \n",
    "    # OBTAIN TRAINING LABELS\n",
    "\n",
    "    training_labels = label_list[day]\n",
    "    \n",
    "    # DOWNSAMPLING TRAINING\n",
    "    # the model learns quite well on the training set but it cannot extrapolate that well on the testing set\n",
    "    # skip this part\n",
    "    #training_features, training_labels = downsampling(training_features, training_labels)\n",
    "    \n",
    "    # CROSS VALIDATION TO OBTAIN THE EXPECTED ERROR\n",
    "    \n",
    "    print('Calculating Expected ROC AUC')\n",
    "\n",
    "    testing_true_cross_val, testing_pred_cross_val, error_train, total_len_train = cross_validation(obtain_model(model_name), training_features, training_labels)\n",
    "    \n",
    "    # OBTAIN ERROR RATE TRAIN + SIZE\n",
    "    \n",
    "    training_error_rate.append(error_train)\n",
    "    training_size.append(total_len_train)\n",
    "    \n",
    "    print('Error Rate Train', error_train)\n",
    "    print('Error Rate Train Size', total_len_train)\n",
    "    \n",
    "    # CALCULATE EXPECTED ROC AUC\n",
    "\n",
    "    roc_auc_cross_validation = []\n",
    "\n",
    "    # WE GIVE UP ON ROC AUC BECAUSE IT CAN BE THE CASE THAT ONLY ONE CLASS IS REPRESENTED\n",
    "    '''\n",
    "    # CHECK IF THERE WAS ONLY ONE CLASS IN THE TRAINING\n",
    "    if(error_train == 0):\n",
    "        roc_auc_cross_validation.append(1.0)\n",
    "    else:\n",
    "        for i in range(0, len(testing_true_cross_val)):\n",
    "            roc_auc_cross_validation.append(roc_auc_score(testing_true_cross_val[i], testing_pred_cross_val[i]))\n",
    "    \n",
    "    #EXTRACT EXPECTED ROC AUC\n",
    "    \n",
    "    expected_roc_auc_cross_val.append(np.mean(roc_auc_cross_validation))\n",
    "    '''\n",
    "    # FIT MODEL\n",
    "    \n",
    "    print('Model Training')\n",
    "\n",
    "    model = obtain_model(model_name)\n",
    "    model.fit(training_features, training_labels)\n",
    "    \n",
    "    # OBTAIN TESTING DATA + LABELS\n",
    "\n",
    "    day_test = day + 1\n",
    "    \n",
    "    print('Test day', day_test)\n",
    "\n",
    "    # SCALE TESTING DATA\n",
    "\n",
    "    testing_features = scaler.transform(feature_list[day_test])\n",
    "\n",
    "    # OBTAIN TESTING LABELS\n",
    "\n",
    "    testing_labels = label_list[day_test]\n",
    "    \n",
    "    # OBTAIN PREDICTION\n",
    "\n",
    "    predictions_test = model.predict(testing_features)\n",
    "    \n",
    "    # OBTAIN ERROR RATE TEST + SIZE\n",
    "    \n",
    "    testing_err = np.count_nonzero(testing_labels != predictions_test)\n",
    "    testing_error_rate.append(testing_err)\n",
    "    testing_size.append(len(testing_labels))\n",
    "    \n",
    "    print('Error Rate Test', testing_err)\n",
    "    print('Error Rate Test Size', len(testing_labels))\n",
    "    \n",
    "    # GIVE UP BECAUSE OF REASONS LISTED ABOVE\n",
    "    # CALCULATE ACTUAL ROC AUC\n",
    "    '''\n",
    "    print('Calculating Actual ROC AUC')\n",
    "\n",
    "    actual_roc_auc.append(roc_auc_score(testing_labels, predictions_test))\n",
    "    '''\n",
    "    # EXTRACT FEATURE IMPORTANCE\n",
    "    \n",
    "    feature_importance.append(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0464041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expected_actual['Days'] = days\n",
    "df_expected_actual['Training_Error_Rate'] = training_error_rate\n",
    "df_expected_actual['Testing_Error_Rate'] = testing_error_rate\n",
    "df_expected_actual['Training_Size'] = training_size\n",
    "df_expected_actual['Testing_Size'] = testing_size\n",
    "df_expected_actual['Feature_Importance'] = feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13caf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expected_actual.to_csv('./results/concept_drift_job_rf_feature_importance_rs1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf857b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530c4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e47fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194b5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81900c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da06a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2929a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
