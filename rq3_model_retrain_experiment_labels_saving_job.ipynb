{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6525ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_seeds = ['1234', '4887', '597', '1959', '413', '44', '2969', '4971', '4913', '9591']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_intervals(dataset):\n",
    "    '''\n",
    "    Generate interval terminals, so that samples in each interval have:\n",
    "        interval_i = (timestamp >= terminal_i) and (timestamp < terminal_{i+1})\n",
    "\n",
    "    Args:\n",
    "        dataset (chr): Assuming only Backblaze (b) and Google (g) datasets exists\n",
    "    '''\n",
    "    if dataset == 'g':\n",
    "        # time unit in Google: millisecond, tracing time: 29 days\n",
    "        start_time = 604046279\n",
    "        unit_period = 24 * 60 * 60 * 1000 * 1000  # unit period: one day\n",
    "        end_time = start_time + 28*unit_period\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: week, tracing time: one year (50 weeks)\n",
    "        start_time = 1\n",
    "        unit_period = 7  # unit period: one week (7 days)\n",
    "        end_time = start_time + 50*unit_period\n",
    "    # original 1 month\n",
    "    '''\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: month, tracing time: one year (12 months)\n",
    "        start_time = 1\n",
    "        unit_period = 1  # unit period: one month\n",
    "        end_time = start_time + 12*unit_period\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # add one unit for the open-end of range function\n",
    "    terminals = [i for i in range(start_time, end_time+unit_period, unit_period)]\n",
    "\n",
    "    return terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69acaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_model(model_name):\n",
    "    '''\n",
    "    This function instantiate a specific model \n",
    "    Note: the MODEL_TYPE global variable must be set first\n",
    "    Args:\n",
    "        model_name (str): [rf, nn, svm, cart, rgf]\n",
    "    Returns:\n",
    "        (instance): instance of given model with preset parameters.\n",
    "        Return None if the model name is not in the option\n",
    "    '''\n",
    "    if model_name == 'rf':\n",
    "        return RandomForestClassifier(n_estimators=50, criterion='gini', class_weight=None, max_depth=None, \n",
    "                                      min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, \n",
    "                                      n_jobs=N_WORKERS, random_state = random_seed)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84181c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_model_tuned(model_name, dataset):\n",
    "    if dataset == 'g':\n",
    "        if model_name == 'rf':\n",
    "            return RandomForestClassifier(n_estimators=165, criterion='gini', bootstrap=True, class_weight='balanced', \n",
    "                                          max_depth=40, max_features='auto', min_samples_leaf=4, min_samples_split=8, \n",
    "                                          n_jobs=N_WORKERS, random_state = random_seed)\n",
    "    elif dataset == 'b':\n",
    "        if model_name == 'rf':\n",
    "            return RandomForestClassifier(n_estimators=160, criterion='gini', bootstrap=False, class_weight='balanced', \n",
    "                                          max_depth=10, max_features='sqrt', min_samples_leaf=4, min_samples_split=8, \n",
    "                                          n_jobs=N_WORKERS, random_state = random_seed)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d205719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_natural_chunks(features, labels, terminals):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(terminals) - 1):\n",
    "        idx = np.logical_and(features[:, 0] >= terminals[i], features[:, 0] < terminals[i + 1])\n",
    "        feature_list.append(features[idx][:, 1:])\n",
    "        label_list.append(labels[idx])\n",
    "    return feature_list, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d06b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(training_features, training_labels, ratio=10):\n",
    "    #return training_features, training_labels\n",
    "\n",
    "    idx_true = np.where(training_labels == True)[0]\n",
    "    idx_false = np.where(training_labels == False)[0]\n",
    "    #print('Before dowmsampling:', len(idx_true), len(idx_false))\n",
    "    idx_false_resampled = resample(idx_false, n_samples=len(idx_true)*ratio, replace=False, random_state = random_seed)\n",
    "    idx_resampled = np.concatenate([idx_false_resampled, idx_true])\n",
    "    idx_resampled.sort()\n",
    "    resampled_features = training_features[idx_resampled]\n",
    "    resampled_labels = training_labels[idx_resampled]\n",
    "    #print('After dowmsampling:', len(idx_true), len(idx_false_resampled))\n",
    "    return resampled_features, resampled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867dad0",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_extraction(model, features_input):\n",
    "    \n",
    "    # extract features and their importances\n",
    "    \n",
    "    feature_importance_ranking = model.feature_importances_\n",
    "    zipped_features = list(zip(feature_importance_ranking, features_input))\n",
    "    sorted_features_zip = sorted(zipped_features, key = lambda x: x[0], reverse = True)\n",
    "    \n",
    "    # extract mean of importances\n",
    "    \n",
    "    importances = [i[0] for i in sorted_features_zip]\n",
    "    mean_importances = np.mean(importances)\n",
    "    \n",
    "    # extract most important features and return\n",
    "    \n",
    "    most_important_features = [i[1] for i in sorted_features_zip if i[0]>= mean_importances]\n",
    "    \n",
    "    return most_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_non_important_features(features_array, features_names, important_features_names):\n",
    "    # transform array into dataframe and attach features\n",
    "    df_features = pd.DataFrame(np.array(features_array), columns = features_names)\n",
    "    \n",
    "    # filter out columns with non-relevant features\n",
    "    df_important_features = df_features[df_features.columns[~df_features.columns.isin(important_features)==0]]\n",
    "    \n",
    "    # transform dataframe with only into features back into array\n",
    "    important_features_array = df_important_features.to_numpy()\n",
    "    \n",
    "    return important_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_drift_detection(reference_data, testing_data):\n",
    "    \n",
    "    # extract distributions from reference and testing data\n",
    "    \n",
    "    distribution_reference = sns.distplot(np.array(reference_data)).get_lines()[0].get_data()[1]\n",
    "    plt.close()\n",
    "    distribution_test = sns.distplot(np.array(testing_data)).get_lines()[0].get_data()[1]\n",
    "    plt.close()\n",
    "    \n",
    "    # apply KS statistical test\n",
    "    \n",
    "    stat_test = stats.kstest\n",
    "    \n",
    "    v, p = stat_test(distribution_reference, distribution_test)\n",
    "    \n",
    "    # check if drift\n",
    "    \n",
    "    if(p<0.05):\n",
    "        drift_alert = 1\n",
    "    else:\n",
    "        drift_alert = 0\n",
    "\n",
    "    return drift_alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10465761",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04462306",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be685d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './datasets/google_job_failure.csv'\n",
    "interval = 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_job_failure = ['User ID', 'Job Name', 'Scheduling Class',\n",
    "                   'Num Tasks', 'Priority', 'Diff Machine', 'CPU Requested', 'Mem Requested', 'Disk Requested',\n",
    "                   'Avg CPU', 'Avg Mem', 'Avg Disk', 'Std CPU', 'Std Mem', 'Std Disk']\n",
    "columns_initial = ['Job ID', 'Status', 'Start Time', 'End Time'] + features_job_failure\n",
    "\n",
    "\n",
    "# READ DATA\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, header=None)\n",
    "df.columns = columns_initial\n",
    "df = df.tail(-1)\n",
    "\n",
    "df = df.drop(['Job ID'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_end_time = False\n",
    "\n",
    "# EXTRACT FEATURES AND LABELS\n",
    "\n",
    "features = df[(['Start Time']+ features_job_failure)].to_numpy()\n",
    "labels = (df['Status']==3).to_numpy()\n",
    "\n",
    "\n",
    "# FEATURES PREPROCESSING\n",
    "offset = (1 if include_end_time else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE USER ID\n",
    "le = preprocessing.LabelEncoder()\n",
    "features[:, 1+offset] = le.fit_transform(features[:, 1+offset])\n",
    "\n",
    "# ENCODE JOB NAME\n",
    "le = preprocessing.LabelEncoder()\n",
    "features[:, 2+offset] = le.fit_transform(features[:, 2+offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac870876",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDE FEATURES INTO DAYS \n",
    "\n",
    "feature_list, label_list = obtain_natural_chunks(features, labels, obtain_intervals('g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081eb8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ea2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['D1_2', 'D2_3', 'D3_4', 'D4_5', 'D5_6', 'D6_7', 'D7_8', 'D8_9', 'D9_10', 'D10_11',\n",
    "        'D11_12', 'D12_13', 'D13_14', 'D14_15', 'D15_16', 'D16_17', 'D17_18', 'D18_19', 'D20_21',\n",
    "        'D22_23', 'D23_24', 'D24_25', 'D25_26', 'D26_27', 'D27_28', 'D28_29', 'D29_30']\n",
    "len(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks = len(feature_list)\n",
    "num_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23eb9f4",
   "metadata": {},
   "source": [
    "## True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_testing_labels = np.hstack(label_list[num_chunks//2:])\n",
    "true_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45103ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309bd4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52049619",
   "metadata": {},
   "source": [
    "# McUDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9bb109",
   "metadata": {},
   "source": [
    "#### Reducing the number of retraining needed and the costs of obtaining labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d37812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detected_drifts = []\n",
    "total_time_training = 0\n",
    "predictions_test_dd2_sc2 = []\n",
    "\n",
    "\n",
    "no_necessary_retrainings = 0\n",
    "necessary_label_annotation_effort = 0\n",
    "overall_total_time_training = 0\n",
    "\n",
    "length_drifts_detected = 0\n",
    "\n",
    "\n",
    "initial_training_batches_list = list(range(0, num_chunks//2))\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "for i in tqdm(range(num_chunks//2, num_chunks)):\n",
    "    \n",
    "    print('Evaluated Period', i + 1)\n",
    "\n",
    "    \n",
    "   # obtain training features and labels\n",
    "\n",
    "    \n",
    "    training_feature_list = [feature_list[i] for i in initial_training_batches_list]\n",
    "    training_label_list = [label_list[i] for i in initial_training_batches_list]\n",
    "    \n",
    "    \n",
    "    training_features_init = np.vstack(training_feature_list)\n",
    "    training_labels_init = np.hstack(training_label_list)\n",
    "    drift_alert = 0\n",
    "    \n",
    "    \n",
    "    # check if it is the first batch\n",
    "    if(i==num_chunks//2):\n",
    "        training_features = training_features_init\n",
    "        training_labels = training_labels_init\n",
    "        current_training_batches_list = initial_training_batches_list.copy()\n",
    "        print('Initial Training Batches', current_training_batches_list)\n",
    "    \n",
    "    \n",
    "    print('Training for Model before Scaling', training_features)\n",
    "    print(len(training_features))\n",
    "    \n",
    "    \n",
    "    # scaler and downsampling for training data\n",
    "    update_scaler = StandardScaler()\n",
    "    training_features_model = update_scaler.fit_transform(training_features)\n",
    "    training_features_model, training_labels_model = downsampling(training_features_model, training_labels)\n",
    "    \n",
    "    # obtain testing features and labels\n",
    "    testing_features = feature_list[i]\n",
    "    testing_labels = label_list[i]\n",
    "    \n",
    "    # scaling testing features\n",
    "    testing_features_model = update_scaler.transform(testing_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # model train and prediction extractions\n",
    "    t = time.time()\n",
    "    update_model_dd = obtain_model_tuned('rf', 'g')\n",
    "    update_model_dd.fit(training_features_model, training_labels_model)\n",
    "    elapsed = time.time() - t\n",
    "    total_time_training = total_time_training + elapsed\n",
    "    \n",
    "    overall_total_time_training = overall_total_time_training + total_time_training\n",
    "\n",
    "    \n",
    "    predictions_test_current = update_model_dd.predict(testing_features_model)\n",
    "    predictions_test_dd2_sc2 = np.concatenate([predictions_test_dd2_sc2, predictions_test_current])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # check for concept drift in the data\n",
    "    \n",
    "    # extract important features\n",
    "    \n",
    "    important_features = important_features_extraction(update_model_dd, features_job_failure)\n",
    "    print('Important Features', important_features)\n",
    "    \n",
    "    # filter non-important features from train and test\n",
    "    \n",
    "    training_important_features_model = filtering_non_important_features(training_features_model, features_job_failure, important_features)\n",
    "    testing_important_features_model = filtering_non_important_features(testing_features_model, features_job_failure, important_features)\n",
    "\n",
    "    #print(\"TRAINING IMPORTANT FEATURES\", training_important_features_model)\n",
    "    #print(\"LEN TRAINING IMPORTANT FEATURES\", len(training_important_features_model[0]))\n",
    "    #print(\"LEN Testing IMPORTANT FEATURES\", len(testing_important_features_model[0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    drift_alert = ks_drift_detection(training_important_features_model, testing_important_features_model)\n",
    "    detected_drifts.append(drift_alert)\n",
    "    \n",
    "    if(drift_alert==1):\n",
    "        \n",
    "        length_drifts_detected = length_drifts_detected + len(testing_labels)\n",
    "\n",
    "        \n",
    "        print('CHANGE OF TRAINING AT ', i - num_chunks//2 + 1)\n",
    "        \n",
    "        no_necessary_retrainings = no_necessary_retrainings + 1\n",
    "        necessary_label_annotation_effort = necessary_label_annotation_effort + len(testing_labels)\n",
    "    \n",
    "        current_training_batches_list.remove(current_training_batches_list[0])        \n",
    "        current_training_batches_list.append(i)\n",
    "        \n",
    "        print('Current Training Batches',current_training_batches_list)\n",
    "        \n",
    "        \n",
    "        training_features_list_updated = [feature_list[i] for i in current_training_batches_list]\n",
    "        training_labels_list_updated = [label_list[i] for i in current_training_batches_list]\n",
    "        \n",
    "        training_features = np.vstack(training_features_list_updated)\n",
    "        training_labels = np.hstack(training_labels_list_updated)\n",
    "        \n",
    "        \n",
    "        print('New Training', training_features)\n",
    "        print(len(training_features))\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('NO CHANGE')\n",
    "        print('Current Training Batches',current_training_batches_list)\n",
    "        \n",
    "        #training_features = np.vstack(feature_list[i - num_chunks//2: i])\n",
    "        #training_labels = np.hstack(label_list[i - num_chunks//2: i])\n",
    "\n",
    "end = time.time() - begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ec6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(true_testing_labels, predictions_test_dd2_sc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_mcudi_sc2 = pd.DataFrame(columns=['Random_Seed', 'Model', 'Scenario', 'Drifts', 'ROC_AUC', 'Run_Time', 'Drifts_Detected', 'Label_Costs'])\n",
    "df_results_mcudi_sc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd98dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_mcudi_sc2.loc[0] = [random_seed, 'McUDI', '2', str(no_necessary_retrainings)+'/'+str(len(detected_drifts)), roc_auc_score(true_testing_labels, predictions_test_dd2_sc2), end, detected_drifts, length_drifts_detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_mcudi_sc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633c6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc41515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_job_mean = pd.DataFrame()\n",
    "df_results_job_mean = pd.concat([df_results_job_mean, df_results_mcudi_sc2])\n",
    "df_results_job_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8545eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_job_mean = df_results_job_mean.reset_index(drop=True)\n",
    "df_results_job_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec983ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0446ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_job_mean.to_csv('./results/df_results_job_performance_mcudi_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dc41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
