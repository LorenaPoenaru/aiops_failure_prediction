{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73148363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_seeds = ['1234', '4887', '597', '1959', '413', '44', '2969', '4971', '4913', '9591']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_intervals(dataset):\n",
    "    '''\n",
    "    Generate interval terminals, so that samples in each interval have:\n",
    "        interval_i = (timestamp >= terminal_i) and (timestamp < terminal_{i+1})\n",
    "\n",
    "    Args:\n",
    "        dataset (chr): Assuming only Backblaze (b) and Google (g) datasets exists\n",
    "    '''\n",
    "    if dataset == 'g':\n",
    "        # time unit in Google: millisecond, tracing time: 29 days\n",
    "        start_time = 604046279\n",
    "        unit_period = 24 * 60 * 60 * 1000 * 1000  # unit period: one day\n",
    "        end_time = start_time + 28*unit_period\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: week, tracing time: one year (50 weeks)\n",
    "        start_time = 1\n",
    "        unit_period = 7  # unit period: one week (7 days)\n",
    "        end_time = start_time + 50*unit_period\n",
    "    # original 1 month\n",
    "    '''\n",
    "    elif dataset == 'b':\n",
    "        # time unit in Backblaze: month, tracing time: one year (12 months)\n",
    "        start_time = 1\n",
    "        unit_period = 1  # unit period: one month\n",
    "        end_time = start_time + 12*unit_period\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # add one unit for the open-end of range function\n",
    "    terminals = [i for i in range(start_time, end_time+unit_period, unit_period)]\n",
    "\n",
    "    return terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69acaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_model(model_name):\n",
    "    '''\n",
    "    This function instantiate a specific model \n",
    "    Note: the MODEL_TYPE global variable must be set first\n",
    "    Args:\n",
    "        model_name (str): [rf, nn, svm, cart, rgf]\n",
    "    Returns:\n",
    "        (instance): instance of given model with preset parameters.\n",
    "        Return None if the model name is not in the option\n",
    "    '''\n",
    "    if model_name == 'rf':\n",
    "        return RandomForestClassifier(n_estimators=50, criterion='gini', class_weight=None, max_depth=None, \n",
    "                                      min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, \n",
    "                                      n_jobs=N_WORKERS, random_state = random_seed)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84181c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_model_tuned(model_name, dataset):\n",
    "    if dataset == 'g':\n",
    "        if model_name == 'rf':\n",
    "            return RandomForestClassifier(n_estimators=165, criterion='gini', bootstrap=True, class_weight='balanced', \n",
    "                                          max_depth=40, max_features='auto', min_samples_leaf=4, min_samples_split=8, \n",
    "                                          n_jobs=N_WORKERS, random_state = random_seed)\n",
    "    elif dataset == 'b':\n",
    "        if model_name == 'rf':\n",
    "            return RandomForestClassifier(n_estimators=160, criterion='gini', bootstrap=False, class_weight='balanced', \n",
    "                                          max_depth=10, max_features='sqrt', min_samples_leaf=4, min_samples_split=8, \n",
    "                                          n_jobs=N_WORKERS, random_state = random_seed)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d205719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_natural_chunks(features, labels, terminals):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(terminals) - 1):\n",
    "        idx = np.logical_and(features[:, 0] >= terminals[i], features[:, 0] < terminals[i + 1])\n",
    "        feature_list.append(features[idx][:, 1:])\n",
    "        label_list.append(labels[idx])\n",
    "    return feature_list, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d06b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(training_features, training_labels, ratio=10):\n",
    "    #return training_features, training_labels\n",
    "\n",
    "    idx_true = np.where(training_labels == True)[0]\n",
    "    idx_false = np.where(training_labels == False)[0]\n",
    "    #print('Before dowmsampling:', len(idx_true), len(idx_false))\n",
    "    idx_false_resampled = resample(idx_false, n_samples=len(idx_true)*ratio, replace=False, random_state = random_seed)\n",
    "    idx_resampled = np.concatenate([idx_false_resampled, idx_true])\n",
    "    idx_resampled.sort()\n",
    "    resampled_features = training_features[idx_resampled]\n",
    "    resampled_labels = training_labels[idx_resampled]\n",
    "    #print('After dowmsampling:', len(idx_true), len(idx_false_resampled))\n",
    "    return resampled_features, resampled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584a6f0",
   "metadata": {},
   "source": [
    "Feature Importance Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5795ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_extraction(model, features_input):\n",
    "    \n",
    "    # extract features and their importances\n",
    "    \n",
    "    feature_importance_ranking = model.feature_importances_\n",
    "    zipped_features = list(zip(feature_importance_ranking, features_input))\n",
    "    sorted_features_zip = sorted(zipped_features, key = lambda x: x[0], reverse = True)\n",
    "    \n",
    "    # extract mean of importances\n",
    "    \n",
    "    importances = [i[0] for i in sorted_features_zip]\n",
    "    mean_importances = np.mean(importances)\n",
    "    \n",
    "    # extract most important features and return\n",
    "    \n",
    "    most_important_features = [i[1] for i in sorted_features_zip if i[0]>= mean_importances]\n",
    "    \n",
    "    return most_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecbc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_non_important_features(features_array, features_names, important_features_names):\n",
    "    # transform array into dataframe and attach features\n",
    "    df_features = pd.DataFrame(np.array(features_array), columns = features_names)\n",
    "    \n",
    "    # filter out columns with non-relevant features\n",
    "    df_important_features = df_features[df_features.columns[~df_features.columns.isin(important_features)==0]]\n",
    "    \n",
    "    # transform dataframe with only into features back into array\n",
    "    important_features_array = df_important_features.to_numpy()\n",
    "    \n",
    "    return important_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_drift_detection(reference_data, testing_data):\n",
    "    \n",
    "    # extract distributions from reference and testing data\n",
    "    \n",
    "    distribution_reference = sns.distplot(np.array(reference_data)).get_lines()[0].get_data()[1]\n",
    "    plt.close()\n",
    "    distribution_test = sns.distplot(np.array(testing_data)).get_lines()[0].get_data()[1]\n",
    "    plt.close()\n",
    "    \n",
    "    # apply KS statistical test\n",
    "    \n",
    "    stat_test = stats.kstest\n",
    "    \n",
    "    v, p = stat_test(distribution_reference, distribution_test)\n",
    "    \n",
    "    # check if drift\n",
    "    \n",
    "    if(p<0.05):\n",
    "        drift_alert = 1\n",
    "    else:\n",
    "        drift_alert = 0\n",
    "\n",
    "    return drift_alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a2a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10465761",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04462306",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be685d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../Documents/phd_related/data_sets_concept_drift/AIOps_failure_prediction/disk_failure_2015.csv'\n",
    "interval = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_disk_failure = ['smart_1_raw', 'smart_4_raw', 'smart_5_raw', 'smart_7_raw', 'smart_9_raw', 'smart_12_raw', 'smart_187_raw', 'smart_193_raw', 'smart_194_raw', 'smart_197_raw', 'smart_199_raw', \n",
    "                         'smart_4_raw_diff', 'smart_5_raw_diff', 'smart_9_raw_diff', 'smart_12_raw_diff', 'smart_187_raw_diff', 'smart_193_raw_diff', 'smart_197_raw_diff', 'smart_199_raw_diff']\n",
    "columns = ['serial_number', 'date'] + features_disk_failure + ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c156e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_disk_failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH, header=None)\n",
    "# put columns names\n",
    "df.columns = columns\n",
    "# ignore serial number\n",
    "df = df[df.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform date to date time\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a942b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide on days of year\n",
    "\n",
    "# original implementation\n",
    "#df['date'] = pd.Series(pd.DatetimeIndex(df['date']).month)\n",
    "\n",
    "# divide on weeks\n",
    "df['date'] = pd.Series(pd.DatetimeIndex(df['date']).day_of_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ac2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[df.columns[:-1]].to_numpy()\n",
    "labels = df[df.columns[-1]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36695cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDE FEATURES INTO WEEKS \n",
    "\n",
    "feature_list, label_list = obtain_natural_chunks(features, labels, obtain_intervals('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original implementation\n",
    "#months = ['M1_2', 'M2_3', 'M3_4', 'M4_5', 'M5_6', 'M6_7', 'M7_8', 'M8_9', 'M9_10', 'M10_11', 'M11_12']\n",
    "\n",
    "# divide on weeks\n",
    "weeks = []\n",
    "for i in range(0, len(feature_list)-1):\n",
    "    string_week = 'W' + str(i+1) + '_' + str(i+2)\n",
    "    weeks.append(string_week)\n",
    "len(weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks = len(feature_list)\n",
    "num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332a6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23eb9f4",
   "metadata": {},
   "source": [
    "## True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_testing_labels = np.hstack(label_list[num_chunks//2:])\n",
    "true_testing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4461e4",
   "metadata": {},
   "source": [
    "# DF Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_disk = pd.DataFrame()\n",
    "df_results_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01d405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a8cb68c",
   "metadata": {},
   "source": [
    "# Building Static Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029a309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "begin = time.time()\n",
    "\n",
    "# extracting training features and labels\n",
    "training_features = np.vstack(feature_list[0: num_chunks//2])\n",
    "training_labels = np.hstack(label_list[0: num_chunks//2])\n",
    "\n",
    "# scaling training data\n",
    "scaler = StandardScaler()\n",
    "training_features = scaler.fit_transform(training_features)\n",
    "\n",
    "# downsampling training data\n",
    "training_features_downsampling, training_labels_downsampling = downsampling(training_features, training_labels)\n",
    "\n",
    "# training model\n",
    "t = time.time()\n",
    "static_model = obtain_model_tuned('rf', 'b')\n",
    "static_model.fit(training_features_downsampling, training_labels_downsampling)\n",
    "elapsed = time.time() - t\n",
    "print('Training time: ', elapsed)\n",
    "\n",
    "total_time_training = 0\n",
    "predictions_test_static_model = []\n",
    "\n",
    "# true testing labels\n",
    "true_testing_labels = np.hstack(label_list[num_chunks//2:])\n",
    "\n",
    "\n",
    "# lengths of tests\n",
    "len_test = 0\n",
    "\n",
    "for i in tqdm(range(num_chunks//2, num_chunks)):\n",
    "    \n",
    "    # obtain testing features and labels\n",
    "    testing_features = feature_list[i]\n",
    "    #len_test = len_test + len(testing_features)\n",
    "    testing_labels = label_list[i]\n",
    "    \n",
    "    # scaling testing features\n",
    "    testing_features = scaler.transform(testing_features)\n",
    "    \n",
    "    # evaluate model on testing data\n",
    "    print('Testing models on period', i + 1)\n",
    "    predictions_test_updated = static_model.predict(testing_features)\n",
    "    predictions_test_static_model = np.concatenate([predictions_test_static_model, predictions_test_updated])\n",
    "    print(len(testing_labels))\n",
    "    print(len(predictions_test_static_model))\n",
    "    \n",
    "end = time.time() - begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_static = pd.DataFrame(columns=['Random_Seed', 'Model', 'Scenario', 'Drifts', 'ROC_AUC', 'Run_Time', 'Drifts_Detected', 'Label_Costs'])\n",
    "df_results_static.loc[0] = [random_seed, 'static', '-', '0/25', roc_auc_score(true_testing_labels, predictions_test_static_model), end, np.zeros(25, dtype=int), 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208fe2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_disk = pd.concat([df_results_disk, df_results_static])\n",
    "df_results_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85ada6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df523163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f64b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dd33e31",
   "metadata": {},
   "source": [
    "# Build Periodical Model Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a3cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_time_training = 0\n",
    "predictions_test = []\n",
    "\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_chunks//2, num_chunks)):\n",
    "    \n",
    "    # obtain training features and labels\n",
    "    training_features_init = np.vstack(feature_list[0: i])\n",
    "    training_labels_init = np.hstack(label_list[0//2: i])\n",
    "    drift_alert = 0\n",
    "    \n",
    "    # check if it is the first batch\n",
    "    if(i==num_chunks//2):\n",
    "        training_features = training_features_init\n",
    "        training_labels = training_labels_init\n",
    "    \n",
    "    # scaler and downsampling for training data\n",
    "    update_scaler = StandardScaler()\n",
    "    training_features = update_scaler.fit_transform(training_features)\n",
    "    training_features, training_labels = downsampling(training_features, training_labels)\n",
    "    \n",
    "    # obtain testing features and labels\n",
    "    testing_features = feature_list[i]\n",
    "    testing_labels = label_list[i]\n",
    "    \n",
    "    # scaling testing features\n",
    "    testing_features = update_scaler.transform(testing_features)\n",
    "    \n",
    "    # train model & track time\n",
    "    t = time.time()\n",
    "    update_model = obtain_model_tuned('rf', 'b')\n",
    "    update_model.fit(training_features, training_labels)\n",
    "    elapsed = time.time() - t\n",
    "    \n",
    "    total_time_training = total_time_training + elapsed\n",
    "    \n",
    "    # evaluate model on testing data\n",
    "    print('Testing models on period', i + 1)\n",
    "    predictions_test_updated = update_model.predict(testing_features)\n",
    "    predictions_test = np.concatenate([predictions_test, predictions_test_updated])\n",
    "    \n",
    "    training_features = np.vstack(feature_list[i + 1 - num_chunks//2: i+1])\n",
    "    training_labels = np.hstack(label_list[i + 1 - num_chunks//2: i+1])\n",
    "\n",
    "end = time.time() - begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9224145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_periodic = pd.DataFrame(columns=['Random_Seed', 'Model', 'Scenario', 'Drifts', 'ROC_AUC', 'Run_Time', 'Drifts_Detected', 'Label_Costs'])\n",
    "#df_results_periodic = pd.DataFrame()\n",
    "df_results_periodic.loc[0] = [random_seed, 'periodic', '-', '25/25', roc_auc_score(true_testing_labels, predictions_test), end, np.ones(25, dtype=int), len(true_testing_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_disk = pd.concat([df_results_disk, df_results_periodic])\n",
    "df_results_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc923c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98c5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e6f6a9",
   "metadata": {},
   "source": [
    "# Build Drift Detection based Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822c81b",
   "metadata": {},
   "source": [
    "### KS on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc733ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detected_drifts = []\n",
    "total_time_training = 0\n",
    "predictions_test_dd_sc1 = []\n",
    "\n",
    "no_necessary_retrainings = 0\n",
    "necessary_label_annotation_effort = 0\n",
    "overall_total_time_training = 0\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "length_drifts_detected = 0\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_chunks//2, num_chunks)):\n",
    "    \n",
    "    print('Evaluated Period', i + 1)\n",
    "    print(i)\n",
    "    print(num_chunks//2)\n",
    "    \n",
    "    # obtain training features and labels\n",
    "    training_features_init = np.vstack(feature_list[0: i])\n",
    "    training_labels_init = np.hstack(label_list[0//2: i])\n",
    "    drift_alert = 0\n",
    "    \n",
    "    # check if it is the first batch\n",
    "    if(i==num_chunks//2):\n",
    "        training_features = training_features_init\n",
    "        training_labels = training_labels_init\n",
    "        \n",
    "    print('Training for Model before Scaling', training_features)\n",
    "    print(len(training_features))\n",
    "    \n",
    "    # scaler and downsampling for training data\n",
    "    update_scaler = StandardScaler()\n",
    "    training_features_model = update_scaler.fit_transform(training_features)\n",
    "    training_features_model, training_labels_model = downsampling(training_features_model, training_labels)\n",
    "    \n",
    "    # obtain testing features and labels\n",
    "    testing_features = feature_list[i]\n",
    "    testing_labels = label_list[i]\n",
    "    \n",
    "    # scaling testing features\n",
    "    testing_features_model = update_scaler.transform(testing_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # model train and prediction extractions\n",
    "    t = time.time()\n",
    "    update_model_dd = obtain_model_tuned('rf', 'b')\n",
    "    update_model_dd.fit(training_features_model, training_labels_model)\n",
    "    elapsed = time.time() - t\n",
    "    total_time_training = total_time_training + elapsed\n",
    "    \n",
    "    overall_total_time_training = overall_total_time_training + total_time_training\n",
    "    \n",
    "    predictions_test_current = update_model_dd.predict(testing_features_model)\n",
    "    predictions_test_dd_sc1 = np.concatenate([predictions_test_dd_sc1, predictions_test_current])\n",
    "    \n",
    "    # check for concept drift in the data\n",
    "    \n",
    "    # extract distributions\n",
    "    distribution_training = sns.distplot(np.array(training_features_model)).get_lines()[0].get_data()[1]\n",
    "    plt.close()\n",
    "    distribution_test = sns.distplot(np.array(testing_features_model)).get_lines()[0].get_data()[1]\n",
    "    plt.close()\n",
    "    \n",
    "    stat_test = stats.kstest\n",
    "    \n",
    "    v, p = stat_test(distribution_training, distribution_test)\n",
    "    if(p<0.05):\n",
    "        detected_drifts.append(1)\n",
    "        drift_alert = 1\n",
    "        \n",
    "        length_drifts_detected = length_drifts_detected + len(testing_labels)\n",
    "        \n",
    "    else:\n",
    "        detected_drifts.append(0)\n",
    "        \n",
    "    # Adjust Training in case of Concept Drift, otherwise keep the previous training\n",
    "    \n",
    "    if(drift_alert==1):\n",
    "        \n",
    "        print('CHANGE OF TRAINING')\n",
    "        \n",
    "        no_necessary_retrainings = no_necessary_retrainings + 1\n",
    "        necessary_label_annotation_effort = necessary_label_annotation_effort + len(testing_labels)\n",
    "        \n",
    "        #new_training_features = np.concatenate([training_features[len(testing_features):], testing_features])\n",
    "        #new_training_labels = np.concatenate([training_labels[len(testing_labels):], testing_labels])\n",
    "        \n",
    "        training_features = np.vstack(feature_list[i + 1 - num_chunks//2: i+1])\n",
    "        training_labels = np.hstack(label_list[i + 1 - num_chunks//2: i+1])\n",
    "        \n",
    "        '''\n",
    "        print('Initial Train', training_features)\n",
    "        print(len(training_features))\n",
    "        print('Train Remaining', training_features[len(testing_features):])\n",
    "        print(len(training_features[len(testing_features):]))\n",
    "        print('Test', testing_features)\n",
    "        print(len(testing_features))\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        print('New Training', training_features)\n",
    "        print(len(training_features))\n",
    "        \n",
    "        #training_features = new_training_features\n",
    "        #training_labels = new_training_labels\n",
    "        \n",
    "        drift_alert = 0\n",
    "        \n",
    "        \n",
    "        #training_features = np.vstack(feature_list[i - num_chunks//2: i])\n",
    "        #training_labels = np.hstack(label_list[i - num_chunks//2: i])\n",
    "        \n",
    "end = time.time() - begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_ede_sc1 = pd.DataFrame(columns=['Random_Seed', 'Model', 'Scenario', 'Drifts', 'ROC_AUC', 'Run_Time', 'Drifts_Detected', 'Label_Costs'])\n",
    "#df_results_periodic = pd.DataFrame()\n",
    "df_results_ede_sc1.loc[0] = [random_seed, 'EDE', '1', str(no_necessary_retrainings)+'/'+str(len(detected_drifts)), roc_auc_score(true_testing_labels, predictions_test_dd_sc1), end, detected_drifts, length_drifts_detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33985ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_disk = pd.concat([df_results_disk, df_results_ede_sc1])\n",
    "df_results_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a6647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd685cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951009f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd42d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57e8dc57",
   "metadata": {},
   "source": [
    "### McUDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81600a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detected_drifts = []\n",
    "total_time_training = 0\n",
    "predictions_test_dd2_sc1 = []\n",
    "\n",
    "no_necessary_retrainings = 0\n",
    "necessary_label_annotation_effort = 0\n",
    "overall_total_time_training = 0\n",
    "\n",
    "length_drifts_detected = 0\n",
    "\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_chunks//2, num_chunks)):\n",
    "    \n",
    "    print('Evaluated Period', i + 1)\n",
    "\n",
    "    \n",
    "    # obtain training features and labels\n",
    "    training_features_init = np.vstack(feature_list[0: i])\n",
    "    training_labels_init = np.hstack(label_list[0//2: i])\n",
    "    drift_alert = 0\n",
    "    \n",
    "    # check if it is the first batch\n",
    "    if(i==num_chunks//2):\n",
    "        training_features = training_features_init\n",
    "        training_labels = training_labels_init\n",
    "\n",
    "        \n",
    "        \n",
    "    print('Training for Model before Scaling', training_features)\n",
    "    print(len(training_features))\n",
    "    \n",
    "    # scaler and downsampling for training data\n",
    "    update_scaler = StandardScaler()\n",
    "    training_features_model = update_scaler.fit_transform(training_features)\n",
    "    training_features_model, training_labels_model = downsampling(training_features_model, training_labels)\n",
    "    \n",
    "    #print('Training for Model after Scaling', training_features_model)\n",
    "    #print(len(training_features_model))\n",
    "    \n",
    "    # obtain testing features and labels\n",
    "    testing_features = feature_list[i]\n",
    "    testing_labels = label_list[i]\n",
    "    \n",
    "    # scaling testing features\n",
    "    testing_features_model = update_scaler.transform(testing_features)\n",
    "    \n",
    "    # model train and prediction extractions\n",
    "    t = time.time()\n",
    "    update_model_dd = obtain_model_tuned('rf', 'b')\n",
    "    update_model_dd.fit(training_features_model, training_labels_model)\n",
    "    elapsed = time.time() - t\n",
    "    total_time_training = total_time_training + elapsed\n",
    "    \n",
    "    overall_total_time_training = overall_total_time_training + total_time_training\n",
    "    \n",
    "    predictions_test_current = update_model_dd.predict(testing_features_model)\n",
    "    predictions_test_dd2_sc1 = np.concatenate([predictions_test_dd2_sc1, predictions_test_current])\n",
    "    \n",
    "    # check for concept drift in the data\n",
    "    \n",
    "    # extract important features\n",
    "    \n",
    "    important_features = important_features_extraction(update_model_dd, features_disk_failure)\n",
    "    print('Important Features', important_features)\n",
    "    print('len imp feats', len(important_features))\n",
    "    \n",
    "    # filter non-important features from train and test\n",
    "    \n",
    "    training_important_features_model = filtering_non_important_features(training_features_model, features_disk_failure, important_features)\n",
    "    testing_important_features_model = filtering_non_important_features(testing_features_model, features_disk_failure, important_features)\n",
    "\n",
    "    #print(\"TRAINING IMPORTANT FEATURES\", training_important_features_model)\n",
    "    #print(\"LEN TRAINING IMPORTANT FEATURES\", len(training_important_features_model[0]))\n",
    "    #print(\"LEN Testing IMPORTANT FEATURES\", len(testing_important_features_model[0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    drift_alert = ks_drift_detection(training_important_features_model, testing_important_features_model)\n",
    "    detected_drifts.append(drift_alert)\n",
    "    \n",
    "    if(drift_alert==1):\n",
    "        \n",
    "        length_drifts_detected = length_drifts_detected + len(testing_labels)\n",
    "        \n",
    "        print('CHANGE OF TRAINING AT ', i - num_chunks//2 + 1)\n",
    "        \n",
    "        no_necessary_retrainings = no_necessary_retrainings + 1\n",
    "        necessary_label_annotation_effort = necessary_label_annotation_effort + len(testing_labels)\n",
    "        \n",
    "        \n",
    "        training_features = np.vstack(feature_list[i + 1 - num_chunks//2: i + 1])\n",
    "        training_labels = np.hstack(label_list[i + 1 - num_chunks//2: i + 1])\n",
    "    \n",
    "        print('New Training', training_features)\n",
    "        print(len(training_features))\n",
    "        \n",
    "end = time.time() - begin\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_mcudi_sc1 = pd.DataFrame(columns=['Random_Seed', 'Model', 'Scenario', 'Drifts', 'ROC_AUC', 'Run_Time', 'Drifts_Detected', 'Label_Costs'])\n",
    "#df_results_periodic = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_mcudi_sc1.loc[0] = [random_seed, 'McUDI', '1', str(no_necessary_retrainings)+'/'+str(len(detected_drifts)), roc_auc_score(true_testing_labels, predictions_test_dd2_sc1), end, detected_drifts, length_drifts_detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_disk = pd.concat([df_results_disk, df_results_mcudi_sc1])\n",
    "df_results_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634bb0c",
   "metadata": {},
   "source": [
    "### ROC AUC Scenario 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7ea24",
   "metadata": {},
   "source": [
    "#### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_disk[df_results_disk.Model=='static'].ROC_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5fc20",
   "metadata": {},
   "source": [
    "#### Periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_disk[df_results_disk.Model=='periodic'].ROC_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e482e6",
   "metadata": {},
   "source": [
    "#### EDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc90a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_disk[df_results_disk.Model=='EDE'].ROC_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8233f",
   "metadata": {},
   "source": [
    "#### McUDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_disk[df_results_disk.Model=='McUDI'].ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34c52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
